# -*- coding: utf-8 -*-
"""project2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Hfa4tw76Wy48ALPiKMl03F1SkkWSo1hE

# CSE 264 - PROJECT 2

## Part 1: Calibration
"""

# importing required packages
!pip install opencv-python-headless
import cv2
import numpy as np
import glob
import os
import matplotlib.pyplot as plt
from google.colab.patches import cv2_imshow

"""First, we inspect the set of chessboard images used for calibration:"""

# reading in chessboard images
dir = '/content/drive/MyDrive/CSE264/project2/calib5/'

calib_imgs = []

for f in os.listdir(dir):
  img = cv2.imread(dir+f)
  calib_imgs.append(img)

# displaying the chessboard images
fig=plt.figure(figsize=(15, 15))
columns = 5
rows = 4
for i in range(1, columns*rows+1):
    if i < columns*rows and i <= len(calib_imgs):
      img = calib_imgs[i-1]
      fig.add_subplot(rows, columns, i)
      plt.xticks(ticks=[])
      plt.yticks(ticks=[])
      plt.imshow(img)
plt.show()

"""Now, we follow the turorial for calibration:"""

# converting BGR to grayscale

grays = []

for item in calib_imgs:
  imgcpy = item.copy()
  gray = cv2.cvtColor(imgcpy, cv2.COLOR_BGR2GRAY)
  grays.append(gray)

# termination criteria
criteria = (cv2.TERM_CRITERIA_EPS + cv2.TERM_CRITERIA_MAX_ITER, 30, 0.001)

# setting up objects with corresponding sizes/dimensions
objp = np.zeros((9*6,3), np.float32)
objp[:,:2] = np.mgrid[0:9,0:6].T.reshape(-1,2)

# containers for obj and image points
objpoints = []
imgpoints = []

for img in grays:

  # getting chessboard coordinages
  ret, corners = cv2.findChessboardCorners(gray, (9,6), None)

  # if successfully got corners, add img and obj points
  if ret:
    objpoints.append(objp)

    cv2.cornerSubPix(gray, corners, (11,11), (-1,-1), criteria)
    imgpoints.append(corners)

    # draw and display corners
    cv2.drawChessboardCorners(img, (9,6), corners, ret)
    cv2_imshow(img)

"""As indicated in the above image, the corner points are correctly identified and we can move on to calibration of the camera."""

# camera calibration

retr, matrix, distortion, rotation, translation = cv2.calibrateCamera(objpoints, imgpoints, img.shape[::-1], None, None)

retr

distortion

rotation

translation

"""We can now undo the distortion:"""

# undoing distortion

# step 1: getting an image and refining the new camera matrix
image = calib_imgs[3]
h, w = image.shape[:2]

newmatrix, roi  = cv2.getOptimalNewCameraMatrix(matrix, distortion, (w,h), 1, (w,h))

# step 2: applying undistortion
distorted = image.copy()
distorted = cv2.undistort(image, matrix, distortion, None, newmatrix)

"""And finally, calculating the reprojection error:"""

# reprojection error estimation

error = 0

for i in range(len(objpoints)):
  imgpts, _ = cv2.projectPoints(objpoints[i], rotation[i], translation[i], matrix, distortion)
  error += cv2.norm(imgpoints[i], imgpts, cv2.NORM_L2)/len(imgpts)

print('reprojection error is: {}'.format(error/len(objpoints)))

newmatrix

matrix

distortion

"""## PART 2: Images
Here, we read in the images for inferring the rotation matrix and translation vector.
"""

!pip install opencv-python-headless

import cv2
import numpy as np
import glob
import os
import matplotlib.pyplot as plt
from google.colab.patches import cv2_imshow

c = []
e = []
f = []
for i in range(1,3):
  c.append(cv2.imread('/content/drive/MyDrive/CSE264/project2/images/c{}.jpg'.format(i), 0))
  e.append(cv2.imread('/content/drive/MyDrive/CSE264/project2/images/e{}.jpg'.format(i), 0))
  f.append(cv2.imread('/content/drive/MyDrive/CSE264/project2/images/f{}.jpg'.format(i), 0))

# undistorting the images:
h, w = f[0].shape[:2]


newMatrix, roi  = cv2.getOptimalNewCameraMatrix(matrix, distortion, (w,h), 1, (w,h))

img1 = f[0].copy()
img2 = f[1].copy()

img1 = cv2.undistort(f[0], matrix, distortion, None, newMatrix)
img2 = cv2.undistort(f[1], matrix, distortion, None, newMatrix)

newMatrix

# FLANN parameters
FLAN_INDEX_KDTREE = 0
index_params = dict(algorithm = FLAN_INDEX_KDTREE, trees = 5)
search_params = dict(checks = 50)

# creating feature points

sift = cv2.SIFT_create()
kp1, des1 = sift.detectAndCompute(img1,None)
kp2, des2 = sift.detectAndCompute(img2,None)

flann = cv2.FlannBasedMatcher(index_params, search_params)
matches = flann.knnMatch(des1, des2, k=2)

good = []
pts1 = []
pts2 = []

# ratio test

for i, (m,n) in enumerate(matches):
  if m.distance < 0.8*n.distance:
    good.append([m])
    pts2.append(kp2[m.trainIdx].pt)
    pts1.append(kp1[m.queryIdx].pt)

len(good)

# finding the Fundamental matrix based on the points
pts1 = np.int32(pts1)
pts2 = np.int32(pts2)
F, mask = cv2.findFundamentalMat(pts1,pts2,cv2.FM_LMEDS)

# We select only inlier points
pts1 = pts1[mask.ravel()==1]
pts2 = pts2[mask.ravel()==1]

F

len(pts1)

# function for drawing epipolar lines
def drawlines(img1,img2,lines,pts1,pts2):
    ''' img1 - image on which we draw the epilines for the points in img2
        lines - corresponding epilines '''
    r,c = img1.shape
    img1 = cv2.cvtColor(img1,cv2.COLOR_GRAY2BGR)
    img2 = cv2.cvtColor(img2,cv2.COLOR_GRAY2BGR)
    counter = 0
    for r,pt1,pt2 in zip(lines,pts1,pts2):
      if counter%30 == 0:
        color = tuple(np.random.randint(0,255,3).tolist())
        x0,y0 = map(int, [0, -r[2]/r[1] ])
        x1,y1 = map(int, [c, -(r[2]+r[0]*c)/r[1] ])
        img1 = cv2.line(img1, (x0,y0), (x1,y1), color,)
        img1 = cv2.circle(img1,tuple(pt1),5,color,15)
        img2 = cv2.circle(img2,tuple(pt2),5,color,15)
      counter+=1
    return img1,img2

# Find epilines corresponding to points in right image (second image) and
# drawing its lines on left image
lines1 = cv2.computeCorrespondEpilines(pts2.reshape(-1,1,2), 2,F)
lines1 = lines1.reshape(-1,3)
img5,img6 = drawlines(img1,img2,lines1,pts1,pts2)

# Find epilines corresponding to points in left image (first image) and
# drawing its lines on right image
lines2 = cv2.computeCorrespondEpilines(pts1.reshape(-1,1,2), 1,F)
lines2 = lines2.reshape(-1,3)
img3,img4 = drawlines(img2,img1,lines2,pts2,pts1)

plt.figure(figsize=(40,30))
plt.subplot(121),plt.imshow(img5)
plt.subplot(122),plt.imshow(img3)
plt.show()

plt.figure(figsize=(40,30))
plt.subplot(121),plt.imshow(img4)
plt.subplot(122),plt.imshow(img6)
plt.show()

# finding Essential matrix

r = cv2.findEssentialMat(pts1, pts2, newMatrix)

E = r[0]

E

# decomposing the Essential matrix

rs = cv2.decomposeEssentialMat(E)

R1 = rs[0]
R2 = rs[1]
tvec = rs[2]

R1, R2, tvec

# inverse of the intrinsic camera matrix K:
kinv = np.linalg.inv(newMatrix)

# converts 2D points in pixel ref frame (pts) to 3D points in camera
# ref frame given camera with intrinsic K-inverse (kinv) with focal l (f)

def convert_pixel2cam(kinv,pts,f):
  xsh = cv2.convertPointsToHomogeneous(pts).reshape(-1,3)
  return (f*np.dot(kinv,xsh.T)).T

# triangulation formula
def triangulate(pts1, pts2, kinv, f, translation, R):
  '''
  given 2D feature points 'pts1' and 'pts2' (left and right correspondigly)
  finds depth of points (p^z) in camera reference frame
  kinv - inverse of K matrix
  translation - is the r^L
  R - rotation from L to R matrix
  returns: np.array of p_z of feature points and their 3D caemra coordinates
  '''
  pz = []
  p = []
  # calculating rR - the optical center of left camera in right camera frame
  rR = translation#-np.dot(R,translation)
  # converting feature points from pixel to camera ref frame
  xl = convert_pixel2cam(kinv, pts1, f)
  xr = convert_pixel2cam(kinv, pts2, f)
  for i in range(pts1.shape[0]):
    numerator = np.dot((f*R[0] - xr[i,0]*R[2]),translation)
    denumerator = np.dot((f*R[0] - xr[i,0]*R[2]),xl[i])
    pzi = (-f)*(numerator/denumerator)
    pz.append(pzi)
    alpha = pzi/xl[i,2]
    p.append([alpha*xl[i,0],alpha*xl[i,1], pzi])

  return np.array(pz), np.array(p).reshape(-1,3)

# triangulating and getting p_z as well as 3D coordinates of the feature points
# in camera reference system
pz, p = triangulate(pts1,pts2,kinv, 3200, tvec, R2)

# converting the 3D feature points to 2D pixel coordinates
xsp = cv2.convertPointsFromHomogeneous((np.dot(newMatrix,p.T)).T).reshape(-1,2)
xsp = np.int32(xsp)
xsp

# projecting the obtained 2D points on the left image
# xsp - 2D pixel coordinates of features obtained from triangulation
# pts1 - feature points obtained from step 2
improj = img1.copy()
improj = cv2.cvtColor(improj,cv2.COLOR_GRAY2BGR)
for i in range(xsp.shape[0]):
  improj = cv2.circle(improj,tuple(xsp[i]),2,(250,0,100),12)
  improj = cv2.circle(improj,tuple(pts1[i]),2,(0,250,100),8)

cv2_imshow(improj)

"""We can see that points obtained from feature selection and triangulation almost exactly match. We can also confirm this by computing the norm distance of the two sets of points and verify that the resulting MSE is indeed small."""

print('''Average absolute difference between feature points from
triangulation and matching:\n''')
cv2.norm(xsp, pts1)/xsp.shape[0]

"""## PART 3: Plane Sweeping

In this section, I implement the plane sweeping method for stereo matching.
"""

# normal to the plane parallel to the first camera's ref. frame
n = np.array([0,0,1])

# plane distances (D) based on triangulated depths
distances = np.linspace(min(pz), max(pz), 20)

distances[0], distances[-1]

# computing the homography (H) matrix
def calculateHmatrix(D, n, R, t, K, kinv):
  '''
  Calculates homography H
  D: distance of the plane
  n: normal to plane
  R: rotation matrix from L to R
  t: translation/ r^L
  K: intrinsic camera matrix
  kinv: K^-1
  returns: H and inverse of H
  '''
  rR = np.dot(-R,t)
  term = np.dot(rR,n.reshape(1,-1))/D
  term = R - term
  H = np.dot(np.dot(K,term),kinv)
  return H, np.linalg.inv(H)


# sum of absolute differences (SAD) functions

def SAD(im1, im2):
  return cv2.norm(im1, im2, normType = 2)

# plane sweeping
warped_imgs = []
SADs = []
planes = []
imleft = img1.copy()

for i, D in enumerate(distances):
  H, hinv = calculateHmatrix(D, n, R2, tvec, newMatrix, kinv)
  warped = cv2.warpPerspective(img2, H, img1.shape)
  warped_imgs.append(warped.T)
  im2 = cv2.boxFilter(warped.T, -1, (15,15))
  im1 = cv2.boxFilter(imleft, -1, (15,15 ))
  SAD = np.abs(im2 - im1)
  planes.append(SAD)

planes = np.array(planes)

# plotting the depth image (min SAD for each pixel)
cv2_imshow(np.min(planes, axis = 0))

# plotting the warped images - 20 warped images corresponding to 20 planes
fig=plt.figure(figsize=(15, 15))
columns = 5
rows = 4
for i in range(1, columns*rows+1):
    if i < columns*rows and i <= len(warped_imgs):
      img = warped_imgs[i-1]
      fig.add_subplot(rows, columns, i)
      plt.xticks(ticks=[])
      plt.yticks(ticks=[])
      plt.imshow(img)
plt.show()